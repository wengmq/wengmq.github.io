## 简介

- 本文将介绍如何快速搭建一个MCP client和MCP Server，已经实现两者的互通
- 搭建过程参考：
	- [https://github.com/liaokongVFX/MCP-Chinese-Getting-Started-Guide](https://github.com/liaokongVFX/MCP-Chinese-Getting-Started-Guide)

![](assets/Pasted%20image%2020251219153335.png)



## 开发MCP服务器

- 在这一小节中，我们将会实现一个用于简单的MCP服务器。首先，我们先来通过 uv 初始化我们的项目。

> [!NOTE]
> - uv 官方文档：[https://docs.astral.sh/uv/](https://docs.astral.sh/uv/)

- 我是使用mac环境进行验证
```
# 初始化项目
uv init init hello-world 
cd init hello-world 

# 创建虚拟环境并进入虚拟环境
uv venv
source ./.venv/bin/activate

# 安装依赖
uv add "mcp[cli]" httpx openai

```


- 我们在hello-world 目录下创建一个hello_server.py的文件，内容如下：
```python
import httpx
from mcp.server import FastMCP

# # 初始化 FastMCP 服务器
app = FastMCP('ip-info')

@app.tool()
async def ip_info(query: str) -> str:
    """
    使用IPI库的接口获取查询IP的详细信息
    Args:
        query: 要搜索内容
    Returns:
        查询的IP的详细内容
    """
    async with httpx.AsyncClient() as client:
        url = 'http://ipinfo.io/' + query
        response = await client.get(url, timeout=10)
        if response.status_code != 200:
            return 'IP查询失败'
        return response.text
        
# npx -y @modelcontextprotocol/inspector uv run hello_server.py 
if __name__ == "__main__":
    app.run(transport='stdio')
```

- 实现执行的方法非常简单，MCP 为我们提供了一个 `@mcp.tool()` 我们只需要将实现函数用这个装饰器装饰即可。函数名称将作为工具名称，参数将作为工具参数，并通过注释来描述工具与参数，以及返回值。


## 调试 MCP 服务器

此时，我们就完成了 MCP 服务端的编写。下面，我们来使用官方提供的 `Inspector` 可视化工具来调试我们的服务器。

> 请先确保已经安装了 node 环境。


- 方式1:  通过 npx：

```
npx -y @modelcontextprotocol/inspector <command> <arg1> <arg2>
```


- 我们的这个代码运行命令为：

```shell
npx -y @modelcontextprotocol/inspector uv run hello_server.py
```

- 方式2: 通过mcp：

```shell
mcp dev PYTHONFILE
```

- 我们的这个代码运行命令为：

```shell
mcp dev hello_server.py
```

- 执行成功的情况会自动打开浏览的链接
![](assets/Pasted%20image%2020251218161502.png)

- 点击Run Tool 即执这个工具，可以查看工具执行的返回结果
![](assets/Pasted%20image%2020251218162411.png)



## 开发 MCP 客户端

- 首先，我们先来看看如何在客户端如何调用我们刚才开发的 MCP 服务器中的工具。
- 下面是一个最精简的代码，里面没有实现记录上下文消息等功能，只是为了用最简单的代码来了解如何通过大语言模型来调动 MCP 服务器。这里只演示了如何连接单服务器，如果你期望连接多个 MCP 服务器，无非就是循环一下 `connect_to_server` 中的代码，可以将他们封装成一个类，然后将所有的 MCP 服务器中的工具循环遍历生成一个大的 `available_tools`，然后在通过大语言模型的返回结果进行调用即可，这里就不再赘述了。

> 可以参考官方案例：[https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py)

- hello_server.py的同目录下创建一个hello_client.py文件，这client的客户端其实最核心的流程有：
	- 接受客户的查询输入，并且负责和AI大模型的接口进行对话
	- 在和AI大模型对话的时候会附上所有 mcp 服务器工具列表信息(list_tools)
	- 根据AI大模型的返回的结果判断是否需要使用某个mcp的工具，如果需要使用则调用对应的mcp 服务器的工具(tool_calls)，并且获取工具的的返回结果
	- 再把mcp服务器工具的返回结果的消息继续追加再原本的消息流中，进一步获取AI大模型的结果。

```python
import json
import asyncio
import os
from typing import Optional
from contextlib import AsyncExitStack

from openai import OpenAI
from dotenv import load_dotenv

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


load_dotenv()


class MCPClient:
    def __init__(self):
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.client = OpenAI()

    # 添加 connect_to_server 方法来初始化我们的 MCP 服务器的 session
    async def connect_to_server(self):
        server_params = StdioServerParameters(
            command='uv',
            args=['run', 'hello_server.py'],
            env=None
        )

        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params))
        stdio, write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(stdio, write))

        await self.session.initialize()

    # 我们再实现一个用于调用 MCP 服务器的方法来处理和 DeepSeek 之间的交互
    async def process_query(self, query: str) -> str:
        # 这里需要通过 system prompt 来约束一下大语言模型，
        # 否则会出现不调用工具，自己乱回答的情况
        system_prompt = (
            "You are a helpful assistant."
            "You have the function of ip_info. "
        )
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ]

        # 获取所有 mcp 服务器 工具列表信息
        response = await self.session.list_tools()
        # 生成 function call 的描述信息
        available_tools = [{
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "input_schema": tool.inputSchema
            }
        } for tool in response.tools]

        # 请求 deepseek，function call 的描述信息通过 tools 参数传入
        response = self.client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL"),
            messages=messages,
            tools=available_tools
        )

        # 处理返回的内容
        content = response.choices[0]
        if content.finish_reason == "tool_calls":
            # 如何是需要使用工具，就解析工具
            tool_call = content.message.tool_calls[0]
            tool_name = tool_call.function.name
            tool_args = json.loads(tool_call.function.arguments)

            # 执行工具
            result = await self.session.call_tool(tool_name, tool_args)
            print(f"\n\n[Calling tool {tool_name} with args {tool_args}]\n\n")
			
            # 将 deepseek 返回的调用哪个工具数据和工具执行完成后的数据都存入messages中
            messages.append(content.message.model_dump())
            messages.append({
                "role": "tool",
                "content": result.content[0].text,
                "tool_call_id": tool_call.id,
            })

            # 将上面的结果再返回给 deepseek 用于生产最终的结果
            response = self.client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL"),
                messages=messages,
            )
            return response.choices[0].message.content

        return content.message.content

    # 实现循环提问
    async def chat_loop(self):
        while True:
            try:
                query = input("\nQuery: ").strip()

                if query.lower() == 'quit':
                    break

                response = await self.process_query(query)
                print("\n" + response)

            except Exception as e:
                import traceback
                traceback.print_exc()
                
    # 最后退出后关闭session
    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()


async def main():
    client = MCPClient()
    try:
        await client.connect_to_server()
        await client.chat_loop()
    finally:
        await client.cleanup()


if __name__ == "__main__":
    import sys
    asyncio.run(main())

```

- 运行脚本：
```
	python ./hello_client.py
```

![](assets/Pasted%20image%2020251218173358.png)


## 本地加载MCP Server

- 上面使用的ip-info这个mcp server，如何在你每次和AI对话的时候自动调用，而不是使用我们的hello_client.py去调用？
- 其实市面上有很多的AI的桌面应用可以支持本地AI对话的时候加入MCP的服务器，我们不用每次自己每次去编写一个client文件去处理， 官方是推荐使用claude桌面端来连接到本地的MCP服务器（[地址](https://modelcontextprotocol.io/docs/develop/connect-local-servers)）：
	- Claude 桌面端
	- VsCode 的 cline 插件
	- Cursor
	- 等等

- 例如使用claude 连接你的本地的MCP server，设置你的cluad客户端的配置文件
	- 这里的/Users/wengmq/wengmq/bs/ens-mcp/mcp-demo/hello-world/.venv/bin/python是你的虚拟机环境的python的绝对路径
	- /Users/wengmq/wengmq/bs/ens-mcp/mcp-demo/hello-world/hello_server.py 是你的mcp的服务器的启动py文件入口

![](assets/Pasted%20image%2020251218202319.png)

```json
{
  "mcpServers": {
    "ip_info": {
      "command": "/Users/wengmq/wengmq/bs/ens-mcp/mcp-demo/hello-world/.venv/bin/python",
      "args": [
        "/Users/wengmq/wengmq/bs/ens-mcp/mcp-demo/hello-world/hello_server.py"
      ]
    }
  }
}
```


- 然后再重启claude客户端，下次你再和claude的时候就会自动调用这个ip_info工具了
![](assets/Pasted%20image%2020251218202759.png)

- 使用cursor连接本地的mcp也是类似的
![](assets/Pasted%20image%2020251218202918.png)